{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conectando ao Google Drive, se necessário. Ou então rodar local. Descomentar os caminhos nas células."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = \"Google Drive\"\n",
    "#environment = \"Local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "if environment = \"Google Drive\":\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre - Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando JSON: 100%|██████████| 6783/6783 [00:00<00:00, 18363.45it/s]\n",
      "Escrevendo palavras: 100%|██████████| 6783/6783 [00:00<00:00, 42466.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processamento concluído.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Definir o ambiente e os caminhos\n",
    "environment = \"Local\" \n",
    "environment = \"Google Drive\" # Altere para \"Google Drive\" se estiver usando o Google Drive\n",
    "\n",
    "if environment == \"Google Drive\":\n",
    "    # Caminhos para o Google Drive\n",
    "    input_path = '/content/drive/MyDrive/ADAN/ADAN_notebooks/DBLP/dataset/dblp.json'\n",
    "    output_dir = '/content/drive/MyDrive/ADAN/ADAN_notebooks/DBLP/data/'\n",
    "else:\n",
    "    # Caminhos locais\n",
    "    input_path = '/home/neiruto/Documents/ueg/andsol/adan/repositories/DBLP/dataset/dblp.json'\n",
    "    output_dir = '/home/neiruto/Documents/ueg/andsol/adan/repositories/DBLP/data/'\n",
    "\n",
    "# Verificar e criar o diretório de saída se ele não existir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Dicionários para armazenar os dados\n",
    "papers = {}\n",
    "authors = {}\n",
    "venues = {}\n",
    "word = {}\n",
    "\n",
    "# Inicializar IDs\n",
    "keyid = 0\n",
    "\n",
    "# Expressão regular para limpar texto\n",
    "r = '[!“”\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~—～]+'\n",
    "stopword = ['at', 'based', 'in', 'of', 'for', 'on', 'and', 'to', 'an', 'using', 'with', 'the']\n",
    "\n",
    "# Abrir arquivos de saída\n",
    "f1 = open(os.path.join(output_dir, 'paper_author.txt'), 'w', encoding='utf-8')\n",
    "f2 = open(os.path.join(output_dir, 'paper_venue.txt'), 'w', encoding='utf-8')\n",
    "f3 = open(os.path.join(output_dir, 'paper_word.txt'), 'w', encoding='utf-8')\n",
    "f4 = open(os.path.join(output_dir, 'paper_title.txt'), 'w', encoding='utf-8')\n",
    "f5 = open(os.path.join(output_dir, 'paper_author_names.txt'), 'w', encoding='utf-8')  # Arquivo para nomes de autores\n",
    "f6 = open(os.path.join(output_dir, 'paper_abstract.txt'), 'w', encoding='utf-8')  # Arquivo para abstracts\n",
    "f7 = open(os.path.join(output_dir, 'paper_venue_name.txt'), 'w', encoding='utf-8')  # Novo arquivo para venues e nomes\n",
    "\n",
    "# Carrega os dados do arquivo JSON\n",
    "with open(input_path, 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Processar o JSON com uma barra de progresso\n",
    "with tqdm(total=len(data), desc=\"Processando JSON\") as pbar:\n",
    "    for entry in data:\n",
    "        # Verificar se o ID é numérico e se o label é um inteiro\n",
    "        pid = entry.get('id')\n",
    "        label = entry.get('label')\n",
    "\n",
    "        # Verifica se 'id' e 'label' são inteiros\n",
    "        if not isinstance(pid, int) or not isinstance(label, int):\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "        title = str(entry.get('title', '')).strip()\n",
    "        author = str(entry.get('author', '')).strip()\n",
    "        coauthors = entry.get('coauthors', []) if entry.get('coauthors') else []\n",
    "        abstract = str(entry.get('abstract', '')).strip()\n",
    "        venue = str(entry.get('venue', '')).strip()  # Adicionado para o processamento do venue\n",
    "\n",
    "        # Verificar se o abstract não está vazio\n",
    "        if abstract:\n",
    "            f6.write(f'i{pid}\\t{abstract}\\n')\n",
    "\n",
    "        # Processar autor principal e coautores\n",
    "        all_authors = [author] + coauthors\n",
    "        for author_name in all_authors:\n",
    "            author_clean = author_name.replace(\" \", \"\")\n",
    "            if author_clean not in authors:\n",
    "                authors[author_clean] = keyid\n",
    "                keyid += 1\n",
    "            f1.write(f'i{pid}\\t{authors[author_clean]}\\n')\n",
    "\n",
    "        # Relacionar o paper_id com os nomes dos autores\n",
    "        f5.write(f'i{pid}\\t{\",\".join(all_authors)}\\n')\n",
    "\n",
    "        # Processar venues\n",
    "        if venue not in venues:\n",
    "            venues[venue] = keyid\n",
    "            keyid += 1\n",
    "        f2.write(f'i{pid}\\t{venues[venue]}\\n')\n",
    "\n",
    "        # Processar título\n",
    "        title_cleaned = re.sub(r, ' ', title)\n",
    "        title_cleaned = title_cleaned.replace('\\t', ' ')\n",
    "        title_cleaned = title_cleaned.lower()\n",
    "\n",
    "        f4.write(f'i{pid}\\t{title_cleaned}\\n')\n",
    "\n",
    "        # Processar o arquivo de venue com o nome do paper e o nome da venue\n",
    "        f7.write(f'i{pid}\\t{venue}\\n')\n",
    "\n",
    "        # Processar palavras do título e contar ocorrências\n",
    "        split_cut = title_cleaned.split(' ')\n",
    "        for word_part in split_cut:\n",
    "            if word_part and word_part not in stopword:\n",
    "                if word_part in word:\n",
    "                    word[word_part] += 1\n",
    "                else:\n",
    "                    word[word_part] = 1\n",
    "\n",
    "        pbar.update(1)  # Atualizando a barra de progresso para cada entrada processada\n",
    "\n",
    "# Agora que todas as palavras foram contadas, escrevemos aquelas que aparecem 2 ou mais vezes\n",
    "with tqdm(total=len(data), desc=\"Escrevendo palavras\") as pbar:\n",
    "    for entry in data:\n",
    "        pid = entry.get('id')\n",
    "        title = str(entry.get('title', '')).strip()\n",
    "\n",
    "        # Processar título\n",
    "        title_cleaned = re.sub(r, ' ', title)\n",
    "        title_cleaned = title_cleaned.replace('\\t', ' ')\n",
    "        title_cleaned = title_cleaned.lower()\n",
    "\n",
    "        # Processar palavras do título e escrever se aparecerem mais de 2 vezes\n",
    "        split_cut = title_cleaned.split(' ')\n",
    "        for word_part in split_cut:\n",
    "            if word_part in word and word[word_part] >= 2:\n",
    "                f3.write(f'i{pid}\\t{word_part}\\n')\n",
    "\n",
    "        pbar.update(1)  # Atualizando a barra de progresso para cada entrada processada\n",
    "\n",
    "# Fechar arquivos de saída\n",
    "f1.close()\n",
    "f2.close()\n",
    "f3.close()\n",
    "f4.close()\n",
    "f5.close()\n",
    "f6.close()\n",
    "f7.close()\n",
    "\n",
    "print('Processamento concluído.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geração da rede heterogênea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rede heterogênea salva com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import networkx as nx\n",
    "\n",
    "class HeterogeneousNetworkGenerator:\n",
    "    def __init__(self):\n",
    "        self.G = nx.Graph()  # Rede NetworkX\n",
    "\n",
    "    def read_data(self, dirpath):\n",
    "        # Carregar e processar arquivos de relacionamentos\n",
    "\n",
    "        # Documentos e Palavras\n",
    "        with open(os.path.join(dirpath, \"paper_word.txt\"), encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    paper_id, word = toks\n",
    "                    self.G.add_node(paper_id, type='paper')\n",
    "                    self.G.add_node(word, type='word')\n",
    "                    self.G.add_edge(paper_id, word, relationship='contains')\n",
    "\n",
    "        # Documentos e Autores\n",
    "        with open(os.path.join(dirpath, \"paper_author.txt\"), encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    paper_id, author_id = toks\n",
    "                    self.G.add_node(paper_id, type='paper')\n",
    "                    self.G.add_node(author_id, type='author')\n",
    "                    self.G.add_edge(paper_id, author_id, relationship='written_by')\n",
    "\n",
    "        # Documentos e Conferências\n",
    "        with open(os.path.join(dirpath, \"paper_venue.txt\"), encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    paper_id, conf_id = toks\n",
    "                    self.G.add_node(paper_id, type='paper')\n",
    "                    self.G.add_node(conf_id, type='conference')\n",
    "                    self.G.add_edge(paper_id, conf_id, relationship='presented_at')\n",
    "\n",
    "        # Documentos e Títulos\n",
    "        with open(os.path.join(dirpath, \"paper_title.txt\"), encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                toks = line.strip().split(\"\\t\", 1)\n",
    "                if len(toks) == 2:\n",
    "                    paper_id, title = toks\n",
    "                    title_node_id = f'title_{paper_id}'\n",
    "\n",
    "                    self.G.add_node(title_node_id, type='title', content=title)\n",
    "                    self.G.add_edge(paper_id, title_node_id, relationship='has_title')\n",
    "\n",
    "        # Documentos e Abstracts\n",
    "        with open(os.path.join(dirpath, \"paper_abstract.txt\"), encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                toks = line.strip().split(\"\\t\", 1)\n",
    "                if len(toks) == 2:\n",
    "                    paper_id, abstract = toks\n",
    "                    abstract_node_id = f'abstract_{paper_id}'\n",
    "\n",
    "                    self.G.add_node(abstract_node_id, type='abstract', content=abstract)\n",
    "                    self.G.add_edge(paper_id, abstract_node_id, relationship='has_abstract')\n",
    "\n",
    "        # Documentos e Nomes de Autores\n",
    "        with open(os.path.join(dirpath, \"paper_author_names.txt\"), encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                toks = line.strip().split(\"\\t\", 1)\n",
    "                if len(toks) == 2:\n",
    "                    paper_id, author_names = toks\n",
    "                    author_names_node_id = f'author_names_{paper_id}'\n",
    "\n",
    "                    self.G.add_node(author_names_node_id, type='author_names', content=author_names)\n",
    "                    self.G.add_edge(paper_id, author_names_node_id, relationship='has_author_names')\n",
    "\n",
    "        # Documentos e Organizações (se aplicável)\n",
    "        if os.path.exists(os.path.join(dirpath, \"paper_organization.txt\")):\n",
    "            with open(os.path.join(dirpath, \"paper_organization.txt\"), encoding='utf-8') as file:\n",
    "                for line in file:\n",
    "                    toks = line.strip().split(\"\\t\")\n",
    "                    if len(toks) == 2:\n",
    "                        paper_id, organization = toks\n",
    "                        org_node_id = f'org_{organization}'\n",
    "\n",
    "                        self.G.add_node(org_node_id, type='organization', name=organization)\n",
    "                        self.G.add_edge(paper_id, org_node_id, relationship='affiliated_with')\n",
    "\n",
    "    def save_network(self, filepath):\n",
    "        # Salvar o grafo para uso futuro\n",
    "        with open(filepath, 'wb') as file:\n",
    "            pickle.dump(self.G, file)\n",
    "        print(\"Rede heterogênea salva com sucesso.\")\n",
    "\n",
    "def main():\n",
    "\n",
    "    if environment == \"Google Drive\":\n",
    "        #Google Drive\n",
    "        dirpath = \"/content/drive/MyDrive/ADAN/ADAN_notebooks/DBLP/data\"\n",
    "\n",
    "    else:\n",
    "        #Local\n",
    "        dirpath = \"/home/neiruto/Documents/ueg/andsol/adan/repositories/DBLP/data/\"\n",
    "\n",
    "    filepath = os.path.join(dirpath, \"HeterogeneousNetwork.pkl\")\n",
    "\n",
    "\n",
    "    hng = HeterogeneousNetworkGenerator()\n",
    "    hng.read_data(dirpath)\n",
    "    hng.save_network(filepath)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neiruto/miniconda3/envs/adan/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Extraindo embeddings:   3%|▎         | 190/6783 [09:05<5:15:18,  2.87s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 135\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Parâmetros de features que podem ser incluídas: 'abstract', 'author_names', 'title', 'venue_name', 'word'\u001b[39;00m\n\u001b[1;32m    134\u001b[0m features_to_include \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor_names\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvenue_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Exemplo de como selecionar as features\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m \u001b[43mextract_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_to_include\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 116\u001b[0m, in \u001b[0;36mextract_embeddings\u001b[0;34m(features_to_include)\u001b[0m\n\u001b[1;32m    114\u001b[0m paper_vec \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(documents, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtraindo embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m--> 116\u001b[0m     paper_vec[paper_ids[i]] \u001b[38;5;241m=\u001b[39m \u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Verificar se há vetores zero\u001b[39;00m\n\u001b[1;32m    119\u001b[0m zero_vector_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[4], line 100\u001b[0m, in \u001b[0;36mget_embedding\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     98\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 100\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Usar a média dos embeddings da última camada\u001b[39;00m\n\u001b[1;32m    102\u001b[0m embedding \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/miniconda3/envs/adan/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/adan/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/adan/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1141\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1141\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1153\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1154\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/adan/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/adan/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/adan/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:694\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    683\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    684\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    685\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    691\u001b[0m         output_attentions,\n\u001b[1;32m    692\u001b[0m     )\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 694\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/adan/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/adan/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/adan/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:584\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    574\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    581\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    583\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 584\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/adan/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/adan/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/adan/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:514\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    506\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    512\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    513\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 514\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    524\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/adan/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/adan/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/adan/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:394\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m    383\u001b[0m         hidden_states,\n\u001b[1;32m    384\u001b[0m         attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    389\u001b[0m         output_attentions,\n\u001b[1;32m    390\u001b[0m     )\n\u001b[1;32m    392\u001b[0m bsz, tgt_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m--> 394\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;66;03m# mask needs to be such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m is_cross_attention \u001b[38;5;241m=\u001b[39m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/adan/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/adan/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/adan/lib/python3.12/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "environment = \"Google Drive\"\n",
    "\n",
    "if environment == \"Google Drive\":\n",
    "\n",
    "    # Diretórios e arquivos (Google Drive)\n",
    "    dirpath = \"/content/drive/MyDrive/ADAN/ADAN_notebooks/DBLP/\"\n",
    "    data_dir = os.path.join(dirpath, 'data/')\n",
    "    model_dir = \"/content/drive/MyDrive/ADAN/fine_tuning_sci_bert/output22\"  # Diretório onde está o seu modelo fine-tunado\n",
    "\n",
    "else:\n",
    "    # Diretórios e arquivos (Local)\n",
    "    dirpath = \"/home/neiruto/Documents/ueg/andsol/adan/repositories/DBLP/\"\n",
    "    data_dir = os.path.join(dirpath, 'data/')\n",
    "    model_dir = \"/home/neiruto/Documents/ueg/andsol/adan/scibert/scibert_model/output22\"  # Diretório onde está o seu modelo fine-tunado\n",
    "\n",
    "\n",
    "# Função auxiliar para substituir 'null' ou None por um ponto (.) ou string vazia\n",
    "def clean_value(value, substitute='.'):\n",
    "    if value is None or str(value).lower() == 'null':  # Tratar casos de None e string 'null'\n",
    "        return substitute\n",
    "    return str(value).strip()\n",
    "\n",
    "# Carregar as features com base nos parâmetros escolhidos\n",
    "def load_features(features_to_include):\n",
    "    paperid_abstract = {}\n",
    "    paperid_author_names = {}\n",
    "    paperid_title = {}\n",
    "    paperid_venue_name = {}\n",
    "    paperid_words = {}\n",
    "\n",
    "    # Carregar abstracts\n",
    "    if 'abstract' in features_to_include:\n",
    "        with open(os.path.join(data_dir, \"paper_abstract.txt\"), encoding='utf-8') as abstract_file:\n",
    "            for line in abstract_file:\n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    paperid_abstract[toks[0]] = clean_value(toks[1], '.')\n",
    "\n",
    "    # Carregar nomes dos autores\n",
    "    if 'author_names' in features_to_include:\n",
    "        with open(os.path.join(data_dir, \"paper_author_names.txt\"), encoding='utf-8') as author_file:\n",
    "            for line in author_file:\n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    paperid_author_names[toks[0]] = clean_value(toks[1], ' ')\n",
    "\n",
    "    # Carregar títulos\n",
    "    if 'title' in features_to_include:\n",
    "        with open(os.path.join(data_dir, \"paper_title.txt\"), encoding='utf-8') as title_file:\n",
    "            for line in title_file:\n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    paperid_title[toks[0]] = clean_value(toks[1], '.')\n",
    "\n",
    "    # Carregar nomes de venues\n",
    "    if 'venue_name' in features_to_include:\n",
    "        with open(os.path.join(data_dir, \"paper_venue_name.txt\"), encoding='utf-8') as venue_file:\n",
    "            for line in venue_file:\n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    paperid_venue_name[toks[0]] = clean_value(toks[1], ' ')\n",
    "\n",
    "    # Carregar palavras\n",
    "    if 'word' in features_to_include:\n",
    "        with open(os.path.join(data_dir, \"paper_word.txt\"), encoding='utf-8') as words_file:\n",
    "            for line in words_file:\n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    if toks[0] not in paperid_words:\n",
    "                        paperid_words[toks[0]] = []\n",
    "                    paperid_words[toks[0]].append(clean_value(toks[1], ' '))\n",
    "\n",
    "    return paperid_abstract, paperid_author_names, paperid_title, paperid_venue_name, paperid_words\n",
    "\n",
    "# Função para combinar as features selecionadas\n",
    "def combine_features(paperid_abstract, paperid_author_names, paperid_title, paperid_venue_name, paperid_words, features_to_include):\n",
    "    documents = []\n",
    "    paper_ids = []\n",
    "    for paperid in paperid_title:\n",
    "        abstract = clean_value(paperid_abstract.get(paperid, '.'), ' ') if 'abstract' in features_to_include else ''\n",
    "        author_names = clean_value(paperid_author_names.get(paperid, \"\"), ' ') if 'author_names' in features_to_include else ''\n",
    "        title = clean_value(paperid_title.get(paperid, '.'), ' ') if 'title' in features_to_include else ''\n",
    "        venue_name = clean_value(paperid_venue_name.get(paperid, \"\"), ' ') if 'venue_name' in features_to_include else ''\n",
    "        words = \" \".join([clean_value(word, '.') for word in paperid_words.get(paperid, [])]) if 'word' in features_to_include else ''\n",
    "\n",
    "        combined_text = f\"{abstract} {author_names} {title} {venue_name} {words}\".strip()\n",
    "\n",
    "        if combined_text and not combined_text.isspace():  # Garantir que o documento não está vazio\n",
    "            documents.append(combined_text)\n",
    "            paper_ids.append(paperid)\n",
    "        else:\n",
    "            print(f\"Warning: Empty or invalid document for paper ID {paperid}\")\n",
    "\n",
    "    return documents, paper_ids\n",
    "\n",
    "# Carregar o modelo BERT fine-tunado e o tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModel.from_pretrained(model_dir)\n",
    "\n",
    "# Função para obter embeddings usando o modelo fine-tunado\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Usar a média dos embeddings da última camada\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "    return embedding\n",
    "\n",
    "# Função principal para extrair embeddings\n",
    "def extract_embeddings(features_to_include):\n",
    "    # Carregar features selecionadas\n",
    "    paperid_abstract, paperid_author_names, paperid_title, paperid_venue_name, paperid_words = load_features(features_to_include)\n",
    "\n",
    "    # Combinar features\n",
    "    documents, paper_ids = combine_features(paperid_abstract, paperid_author_names, paperid_title, paperid_venue_name, paperid_words, features_to_include)\n",
    "\n",
    "    # Extrair embeddings para todos os documentos\n",
    "    paper_vec = {}\n",
    "    for i, doc in enumerate(tqdm(documents, desc=\"Extraindo embeddings\")):\n",
    "        paper_vec[paper_ids[i]] = get_embedding(doc)\n",
    "\n",
    "    # Verificar se há vetores zero\n",
    "    zero_vector_count = 0\n",
    "    for paper_id, embedding in paper_vec.items():\n",
    "        if np.all(embedding == 0):\n",
    "            print(f\"Warning: Embedding for paper ID {paper_id} is a zero vector.\")\n",
    "            zero_vector_count += 1\n",
    "\n",
    "    print(f\"Total zero vectors found: {zero_vector_count}\")\n",
    "\n",
    "    # Salvar os embeddings\n",
    "    with open(os.path.join(data_dir, 'scibert_emb.pkl'), \"wb\") as file_obj:\n",
    "        pickle.dump(paper_vec, file_obj)\n",
    "\n",
    "    print(\"Extração de embeddings concluída e embeddings salvos.\")\n",
    "\n",
    "# Parâmetros de features que podem ser incluídas: 'abstract', 'author_names', 'title', 'venue_name', 'word'\n",
    "features_to_include = ['abstract', 'author_names', 'title', 'venue_name', 'word']  # Exemplo de como selecionar as features\n",
    "extract_embeddings(features_to_include)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install torch-geometric\n",
    "#!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.9.0+cu111.html\n",
    "#!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.9.0+cu111.html\n",
    "#!pip install torch-cluster -f https://data.pyg.org/whl/torch-1.9.0+cu111.html\n",
    "#!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-1.9.0+cu111.html\n",
    "#!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Verificar se a GPU está disponível\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando o dispositivo: {device}\")\n",
    "\n",
    "# Diretórios e arquivos\n",
    "dirpath = \"/content/drive/MyDrive/ADAN/ADAN_notebooks/DBLP/\"\n",
    "data_dir = os.path.join(dirpath, 'data/')\n",
    "\n",
    "# Carregar a rede heterogênea\n",
    "with open(os.path.join(data_dir, \"HeterogeneousNetwork.pkl\"), 'rb') as file:\n",
    "    G = pickle.load(file)\n",
    "\n",
    "# Função para carregar embeddings\n",
    "def load_embeddings(embedding_type):\n",
    "    try:\n",
    "        with open(os.path.join(data_dir, f'{embedding_type}_emb.pkl'), \"rb\") as file_obj:\n",
    "            return pickle.load(file_obj)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erro: Arquivo de embeddings {embedding_type} não encontrado.\")\n",
    "        return None\n",
    "\n",
    "def prepare_features(embedding_type, G, embeddings):\n",
    "    nodes = list(G.nodes)\n",
    "    node_idx_map = {node: idx for idx, node in enumerate(nodes)}\n",
    "    edges = [(node_idx_map[u], node_idx_map[v]) for u, v in G.edges]\n",
    "\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous().to(device)\n",
    "\n",
    "    if embeddings:\n",
    "        sample_embedding = next(iter(embeddings.values()))\n",
    "        embedding_dim = sample_embedding.shape[0]\n",
    "        features = []\n",
    "        for node in nodes:\n",
    "            features.append(embeddings.get(node, np.zeros(embedding_dim)))\n",
    "    else:\n",
    "        embedding_dim = 128\n",
    "        features = np.random.normal(loc=0.0, scale=1.0, size=(len(nodes), embedding_dim))\n",
    "\n",
    "    features = np.array(features)\n",
    "    x = torch.tensor(features, dtype=torch.float).to(device)\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index), nodes, embedding_dim\n",
    "\n",
    "# Definir o modelo GCN com quantidade de camadas variável\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_layers):\n",
    "        super(GCN, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GCNConv(input_dim, 512))\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(512, 512))\n",
    "\n",
    "        self.convs.append(GCNConv(512, 512))\n",
    "        self.fc = torch.nn.Linear(512, input_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def train_gcn(data, input_dim, num_layers, epochs=100):\n",
    "    model = GCN(input_dim, num_layers).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "    def train():\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.mse_loss(out, data.x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss = train()\n",
    "        if epoch % 500 == 0 or epoch == epochs - 1:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        new_embeddings = model(data).cpu().numpy()\n",
    "\n",
    "    return new_embeddings\n",
    "\n",
    "def save_embeddings(embeddings, nodes, filename):\n",
    "    embeddings_dict = {node: embeddings[idx] for idx, node in enumerate(nodes)}\n",
    "    with open(filename, \"wb\") as file_obj:\n",
    "        pickle.dump(embeddings_dict, file_obj)\n",
    "\n",
    "# Configurações de experimentos\n",
    "configurations = [\n",
    "    #{\"embedding\": \"tfidf\", \"description\": \"GCN with TF-IDF embeddings\", \"output_dim\": 512, \"num_layers\": 3},\n",
    "    #{\"embedding\": \"word2vec\", \"description\": \"GCN with Word2Vec embeddings\", \"output_dim\": 100, \"num_layers\": 3},\n",
    "    #{\"embedding\": \"random\", \"description\": \"GCN with Random embeddings\", \"output_dim\": 100, \"num_layers\": 3},\n",
    "    {\"embedding\": \"scibert\", \"description\": \"GCN with SciBERT embeddings\", \"output_dim\": 768, \"num_layers\": 3}\n",
    "]\n",
    "\n",
    "# Parâmetros comuns\n",
    "epochs = 10000\n",
    "\n",
    "# Rodar experimentos\n",
    "for config in configurations:\n",
    "    embeddings = load_embeddings(config['embedding'])\n",
    "    if embeddings is None:\n",
    "        continue\n",
    "    data, nodes, input_dim = prepare_features(config['embedding'], G, embeddings)\n",
    "    new_embeddings = train_gcn(data, input_dim=input_dim, num_layers=config[\"num_layers\"], epochs=epochs)\n",
    "\n",
    "    save_path = os.path.join(dirpath, f'final_emb/L3-10000/pemb_final_gcn_{config[\"description\"].replace(\" \", \"_\").lower()}.pkl')\n",
    "    save_embeddings(new_embeddings, nodes, save_path)\n",
    "\n",
    "    print(f\"{config['description']} completed and embeddings saved at {save_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pré - GHAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "raw_path = \"/content/drive/MyDrive/ADAN/ADAN_notebooks/DBLP/dataset/\"\n",
    "\n",
    "# Carregar o JSON principal\n",
    "json_file = raw_path + 'dataset_formatted.json'  # Substitua pelo nome do seu arquivo JSON\n",
    "with open(json_file, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Criar um diretório para armazenar os JSONs de cada autor\n",
    "output_dir = raw_path + 'autores_json/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Função para limpar o nome do autor para ser usado como nome de arquivo\n",
    "def clean_filename(name):\n",
    "    if not name or not isinstance(name, str):\n",
    "        return \"unknown_author\"\n",
    "    # Substituir caracteres inválidos por underscore\n",
    "    return re.sub(r'[\\/:*?\"<>|]', '_', name.replace(' ', '_'))\n",
    "\n",
    "# Separar os dados por autor e salvar em JSONs individuais\n",
    "for entry in data:\n",
    "    author = entry.get('author')\n",
    "\n",
    "    # Verificar se o campo 'author' existe e é uma string válida\n",
    "    if author is None or not isinstance(author, str) or author.strip() == '':\n",
    "        author = 'unknown_author'\n",
    "    else:\n",
    "        author = author.strip()\n",
    "\n",
    "    author_filename = clean_filename(author) + '.json'\n",
    "    json_output = os.path.join(output_dir, author_filename)\n",
    "\n",
    "    # Se o arquivo já existe, adiciona a entrada ao arquivo existente\n",
    "    if os.path.isfile(json_output):\n",
    "        with open(json_output, 'r', encoding='utf-8') as file:\n",
    "            existing_data = json.load(file)\n",
    "\n",
    "        existing_data.append(entry)\n",
    "\n",
    "        with open(json_output, 'w', encoding='utf-8') as file:\n",
    "            json.dump(existing_data, file, indent=4, ensure_ascii=False)\n",
    "    else:\n",
    "        with open(json_output, 'w', encoding='utf-8') as file:\n",
    "            json.dump([entry], file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"JSONs criados para cada nome ambíguo de autor.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
